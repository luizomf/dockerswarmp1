# Manual do V√≠deo: Cluster Docker Swarm na Hostinger

Este documento √© um registro fiel e passo a passo de todas as etapas executadas no v√≠deo para subir o cluster Docker Swarm.

> **Nota:** Este guia assume que voc√™ tem 3 VPS na Hostinger (KVM 2 ou superior) e um dom√≠nio configurado.

## Status Atual
- [ ] VPS Formatadas
- [ ] Acesso SSH Inicial
- [ ] Configura√ß√£o de Rede
- [ ] Swarm Init
- [ ] Deploy

## 0. Prepara√ß√£o e Desmontagem (Opcional)

Caso voc√™ j√° tenha um cluster rodando e queira remover um n√≥ de forma segura para format√°-lo (como estamos fazendo com o `kvm2`), siga estes passos. Isso garante que os servi√ßos sejam migrados para outros n√≥s antes do desligamento.

**No n√≥ gerenciador principal (ex: `kvm8`):**

1. **Drenar o n√≥ (`Drain`):**
   Isso remove todos os cont√™ineres em execu√ß√£o neste n√≥ e os move para outros n√≥s dispon√≠veis. Tamb√©m impede que novas tarefas sejam agendadas nele.
   ```bash
   docker node update kvm2 --availability drain
   ```

2. **Rebaixar para Worker (`Demote`):**
   Se o n√≥ for um Manager, √© uma boa pr√°tica rebaix√°-lo para Worker antes de remov√™-lo. Isso ajuda a manter o qu√≥rum do Swarm est√°vel.
   ```bash
   docker node demote kvm2
   ```

**No n√≥ que ser√° removido (ex: `kvm2`):**

3. **Sair do Cluster:**
   Este comando remove o n√≥ do Swarm e limpa o estado local do Docker Swarm.
   ```bash
   docker swarm leave
   ```

> **Nota:** Repita este mesmo processo para outros n√≥s secund√°rios (como o `kvm4`), deixando o n√≥ principal (`kvm8`) por √∫ltimo.

**De volta ao n√≥ principal (ex: `kvm8`):**

4. **Remover metadados dos n√≥s antigos:**
   Ap√≥s os n√≥s sa√≠rem, eles ficam listados como "Down". Precisamos remov√™-los da lista do gerenciador.
   ```bash
   docker node rm kvm4
   docker node rm kvm2
   ```

5. **Verificar o estado do Cluster:**
   Neste momento, o `kvm8` √© o √∫nico n√≥ restante. Ele √© cr√≠tico pois segura nosso Banco de Dados, o NFS e √© a porta de entrada (Traefik).
   
   Confira se os n√≥s sumiram e o estado atual dos servi√ßos:
   ```bash
   # Deve listar apenas o kvm8 como ativo (Ready/Active)
   docker node ls
   
   # Verifique suas stacks
   docker stack ls
   
   # Verifique onde os servi√ßos est√£o rodando (agora tudo deve estar tentando ir para o kvm8 ou falhando se n√£o houver recursos)
   docker stack services dockerswarmp1
   ```

6. **Remover a Stack:**
   Agora que verificamos tudo, vamos remover a stack (o conjunto de aplica√ß√µes) para garantir que nada fique pendurado ou escrevendo no disco enquanto desligamos.
   ```bash
   docker stack rm dockerswarmp1
   ```
   *(Aguarde alguns instantes para que os containers sejam parados)*

7. **Destruir o Cluster (Swarm Leave Force):**
   Como este √© o √∫ltimo gerenciador (Leader), ele n√£o pode simplesmente "sair" (`leave`). Precisamos for√ßar o encerramento do cluster. **Aten√ß√£o: Isso apaga todas as configura√ß√µes do Swarm, segredos e servi√ßos.**
   ```bash
   docker swarm leave --force
   ```

   **Pronto!** O cluster foi desmontado. Agora as m√°quinas s√£o apenas VPSs comuns com Docker instalado (ou prontas para serem formatadas).

## 1. Formata√ß√£o (Reset Total)

Para garantir um ambiente limpo, formatamos todas as 3 VPSs (`kvm2`, `kvm4`, `kvm8`) usando o painel da Hostinger.

**No hPanel:**
1. Acesse **VPS** > **Gerenciar** (em cada n√≥).
2. V√° em **SO e Painel** > **Sistema Operacional** > **Mudar SO**.
3. Escolha **SO com Aplicativo** e procure por **Docker** (Ubuntu 24.04).
4. Defina uma senha forte para o `root`.

> **Nota:** Existe um v√≠deo anterior detalhando exaustivamente este processo de cria√ß√£o de VPS. Aqui, usamos a imagem pronta "Ubuntu 24.04 with Docker" para ganhar tempo e garantir que o Docker Engine j√° venha instalado e configurado corretamente.

## 2. Configura√ß√£o de Hostname e DNS

Acesse cada VPS via SSH (inicialmente como `root`, usando a senha definida na formata√ß√£o) e configure a identidade da m√°quina.

> **Dica de Acesso R√°pido:**
> No pr√≥prio hPanel, existe um bot√£o **Terminal** (no topo direito da gest√£o da VPS). Ele abre um console web j√° logado como `root` (sem precisar de senha ou chave SSH configurada). √â extremamente √∫til para esses ajustes iniciais antes de configurarmos o nosso acesso SSH definitivo.

**Exemplo no `kvm2`:**

1.  **Definir o Hostname:**
    ```bash
    hostnamectl set-hostname kvm2
    ```

2.  **Ajustar Hosts:**
    Edite o arquivo para associar o IP local ao novo nome e dom√≠nio (FQDN).
    ```bash
    vim /etc/hosts
    ```
    Alterar a linha `127.0.1.1` (ou similar) para ficar assim:
    ```text
    127.0.1.1       kvm2.inprod.cloud       kvm2
    ```

> **Aten√ß√£o aos Dom√≠nios:**
> No v√≠deo, usamos 3 dom√≠nios reais diferentes que j√° possuem apontamentos DNS (Tipo A) criados na Cloudflare apontando para os IPs das VPSs. **Voc√™ precisar√° dos seus pr√≥prios dom√≠nios ou subdom√≠nios.**
>
> **Mapa da Demo:**
> | Hostname | Dom√≠nio | IP Externo | IP VPN (WireGuard) | Usu√°rio SSH |
> | :--- | :--- | :--- | :--- | :--- |
> | **kvm2** | `inprod.cloud` | `76.13.71.178` | `10.100.0.2/24` | `luizotavio` |
> | **kvm4** | `otaviomiranda.cloud` | `191.101.70.130` | `10.100.0.4/24` | `luizotavio` |
> | **kvm8** | `myswarm.cloud` | `89.116.73.152` | `10.100.0.8/24` | `luizotavio` |

> **Dica Hostinger (Alternativa):**
> Voc√™ tamb√©m pode configurar o Hostname diretamente pelo hPanel em **VPS > Configura√ß√µes > Configura√ß√µes de VPS**. O painel valida se o dom√≠nio realmente pertence a voc√™ (ou aponta para a VPS). Se validado, ele configura o hostname automaticamente dentro do sistema operacional, dispensando o comando `hostnamectl`.

*Repita o processo para `kvm4` e `kvm8` ajustando os nomes e dom√≠nios adequados.*

## 3. Firewall de Borda (hPanel)

Antes de configurar o firewall interno (UFW), configuramos o **Firewall da Hostinger** (hPanel) para proteger a rede antes mesmo que o tr√°fego chegue nas VPSs.

A pol√≠tica adotada √© **Whitelist**: Bloqueia tudo (Drop) e libera apenas o necess√°rio.

**Regras Aplicadas (Aplicar em TODAS as VPS):**

| A√ß√£o | Protocolo | Porta | Origem (Source) | Descri√ß√£o |
| :--- | :--- | :--- | :--- | :--- |
| **Accept** | TCP | Any | `187.108.118.25` (Seu IP) | Acesso total do admin (SSH, etc) |
| **Accept** | TCP | Any | `89.116...`, `191.101...`, `76.13...` | Comunica√ß√£o total entre os n√≥s (Swarm TCP) |
| **Accept** | UDP | `4789` | `89.116...`, `191.101...`, `76.13...` | Swarm Overlay Network (VXLAN) |
| **Accept** | UDP | `7946` | `89.116...`, `191.101...`, `76.13...` | Swarm Container Network Discovery |
| **Accept** | ICMP | Any | `89.116...`, `191.101...`, `76.13...` | Ping entre os n√≥s |
| **Accept** | ICMP | Any | `187.108.118.25` (Seu IP) | Ping do admin |
| **Accept** | HTTPS | `443` | `Any` (Qualquer) | Tr√°fego Web Seguro (Traefik) |
| **Accept** | HTTP | `80` | `Any` (Qualquer) | Tr√°fego Web (Traefik) |
| **Accept** | UDP | `51820` | IPs dos N√≥s + Seu IP | WireGuard VPN |
| **Accept** | TCP | `8080` | `187.108.118.25` (Seu IP) | Traefik Dashboard (Dev/Debug) |
| **Drop** | Any | Any | `Any` | **Regra Final: Bloqueia todo o resto** |

> **Nota:** Certifique-se de substituir o IP `187.108.118.25` pelo **SEU IP** atual de internet. Os demais IPs devem ser os IPs das suas outras VPSs.

> **Importante:** Essa configura√ß√£o protege a borda. Ainda assim, configuraremos o `UFW` (firewall local) mais adiante para defesa em profundidade e controle da VPN.

## 4. Cria√ß√£o do Usu√°rio e Docker

Ainda logado como `root` (via Terminal web ou SSH), vamos criar o usu√°rio de trabalho e garantir que ele tenha acesso ao Docker e poderes administrativos.

**Execute em TODAS as VPSs (`kvm2`, `kvm4`, `kvm8`):**

```bash
# Defina o nome do seu usu√°rio
export YOUR_USERNAME="luizotavio"

# Cria o usu√°rio com diret√≥rio home (-m) e shell bash (-s)
useradd -m -s /bin/bash $YOUR_USERNAME

# Adiciona ao grupo 'sudo' (admin) e 'docker' (para rodar docker sem sudo)
usermod -aG sudo $YOUR_USERNAME
usermod -aG docker $YOUR_USERNAME

# Define a senha do usu√°rio
passwd $YOUR_USERNAME

# Teste o acesso mudando para o novo usu√°rio
su $YOUR_USERNAME
```

> **Verifica√ß√£o:** Ap√≥s rodar `su $YOUR_USERNAME`, tente rodar `docker ps`. Se funcionar sem erro de permiss√£o, o grupo `docker` foi aplicado corretamente. Se pedir senha no `sudo`, est√° correto.

## 5. Configura√ß√£o SSH (Chaves e Acesso F√°cil)

Agora vamos configurar o acesso seguro e pr√°tico a partir do **SEU COMPUTADOR**. Isso elimina a necessidade de digitar senhas e IPs o tempo todo.

**1. Gerar par de chaves SSH (no seu PC):**
Se voc√™ ainda n√£o tem uma chave espec√≠fica para este projeto:
```bash
ssh-keygen -t ed25519 -f ~/.ssh/id_hostinger -C "luizotavio"
```

**2. Enviar a chave p√∫blica para as VPSs:**
Isso autoriza sua chave a entrar nos servidores. Repita para cada IP ou dom√≠nio configurado.
```bash
ssh-copy-id -i ~/.ssh/id_hostinger.pub luizotavio@inprod.cloud
ssh-copy-id -i ~/.ssh/id_hostinger.pub luizotavio@otaviomiranda.cloud
ssh-copy-id -i ~/.ssh/id_hostinger.pub luizotavio@myswarm.cloud
```

**3. Criar "apelidos" no `~/.ssh/config` (no seu PC):**
Para n√£o precisar digitar `ssh luizotavio@inprod.cloud` toda hora, vamos criar atalhos (`ssh kvm2`).

Edite ou crie o arquivo `~/.ssh/config`:
```text
Host kvm2
  IgnoreUnknown AddKeysToAgent,UseKeychain
  AddKeysToAgent yes
  HostName inprod.cloud
  User luizotavio
  Port 22
  IdentityFile ~/.ssh/id_hostinger

Host kvm4
  IgnoreUnknown AddKeysToAgent,UseKeychain
  AddKeysToAgent yes
  HostName otaviomiranda.cloud
  User luizotavio
  Port 22
  IdentityFile ~/.ssh/id_hostinger

Host kvm8
  IgnoreUnknown AddKeysToAgent,UseKeychain
  AddKeysToAgent yes
  HostName myswarm.cloud
  User luizotavio
  Port 22
  IdentityFile ~/.ssh/id_hostinger
```

**Teste:**
Agora basta digitar:
```bash
ssh kvm2
```
Se conectar direto, est√° tudo pronto!

## 6. Sudo sem Senha (Opcional/Demo)

Para agilizar o desenvolvimento e evitar digitar a senha de `sudo` repetidamente durante o v√≠deo/configura√ß√£o, vamos configurar o `NOPASSWD`.

> **‚ö†Ô∏è ALERTA DE SEGURAN√áA:**
> Em ambientes de produ√ß√£o cr√≠ticos, isso **n√£o √© recomendado**. Se um atacante ganhar acesso ao seu usu√°rio, ele ganha acesso `root` instantaneamente sem barreiras. Fa√ßa isso apenas se entender o risco ou para ambientes de laborat√≥rio/demo.

**Em cada VPS:**

```bash
# Cria/edita um arquivo espec√≠fico para seu usu√°rio no sudoers.d
sudo visudo -f /etc/sudoers.d/luizotavio
```

Adicione a linha abaixo (trocando `luizotavio` pelo seu usu√°rio):

```text
luizotavio ALL=(ALL) NOPASSWD: ALL
```

Salve e saia. Agora comandos como `sudo apt update` rodar√£o direto.

## 7. Atualiza√ß√£o e Pacotes B√°sicos

Vamos garantir que o sistema esteja seguro, atualizado e com nossas ferramentas favoritas instaladas.

**Execute em TODAS as VPSs:**

```bash
# Define seu fuso hor√°rio (ajuste conforme necess√°rio)
export TIMEZONE='America/Sao_Paulo'

# Atualiza reposit√≥rios e pacotes do sistema
sudo apt update -y
sudo apt upgrade -y

# Instala ferramentas essenciais
# just: task runner (usaremos muito)
# python3/build-essential: para scripts e compila√ß√£o
# acl: para controle fino de permiss√µes
sudo apt install -y vim curl ca-certificates htop python3 \
python3-dev acl build-essential tree just

# Aplica o fuso hor√°rio
sudo timedatectl set-timezone "$TIMEZONE"
```

## 8. Configura√ß√£o do Git

Como vamos clonar o reposit√≥rio e talvez fazer ajustes r√°pidos, configuramos o Git com nossa identidade.

**Execute em TODAS as VPSs:**

```bash
# Ajuste com seus dados
export GIT_USERNAME="luizomf"
export GIT_EMAIL="luizomf@gmail.com"

git config --global user.name "$GIT_USERNAME"
git config --global user.email "$GIT_EMAIL"

# Padroniza√ß√£o de quebra de linha (Linux style - LF)
git config --global core.autocrlf input
git config --global core.eol lf

# Branch padr√£o moderna
git config --global init.defaultbranch main
```

## 9. Hardening do SSH (Blindando o Acesso)

Agora vamos trancar as portas. Desabilitaremos login por senha e acesso de root, deixando apenas nossas chaves autorizadas.

> **‚ö†Ô∏è ALERTA CR√çTICO (Tranqueira √† vista):**
> Este passo **DESATIVA** o login por senha.
> 1. Certifique-se que voc√™ j√° configurou suas chaves SSH (`ssh-copy-id`) no passo anterior e testou o acesso (`ssh kvm2`).
> 2. Se voc√™ rodar isso sem ter a chave configurada, **VOC√ä PERDER√Å O ACESSO SSH** e ter√° que usar o console de emerg√™ncia da Hostinger para consertar.

**Execute em TODAS as VPSs:**

```bash
# Instala o servidor SSH (geralmente j√° vem, mas garante)
sudo apt install -y openssh-server

# Cria nosso arquivo de configura√ß√£o blindada
cat <<-'EOF' | sudo tee "/etc/ssh/sshd_config.d/01_sshd_settings.conf"
###############################################################################
### Start of /etc/ssh/sshd_config.d/01_sshd_settings.conf ######################
###############################################################################

# BLOCK 1: AUTHENTICATION AND ACCESS
PubkeyAuthentication yes            # Apenas chaves
PasswordAuthentication no           # Senhas desativadas (adeus brute-force)
KbdInteractiveAuthentication no     # Sem teclado interativo
ChallengeResponseAuthentication no  # Sem desafio-resposta
PermitRootLogin no                  # Root nunca loga direto
PermitEmptyPasswords no             # Sem senhas vazias
UsePAM yes                          # PAM ativo para sess√£o (mas auth √© s√≥ key)
AuthenticationMethods publickey     # For√ßa auth apenas por chave p√∫blica

# BLOCK 2: ATTACK SURFACE REDUCTION
PermitUserEnvironment no            # Bloqueia inje√ß√£o de env
PermitUserRC no                     # Bloqueia scripts rc de usu√°rio
X11Forwarding no                    # Sem interface gr√°fica remota

# TUNNELING (Ajuste se precisar de t√∫neis)
AllowTcpForwarding no               # Bloqueia t√∫neis TCP
AllowStreamLocalForwarding no       # Bloqueia socket forwarding
AllowAgentForwarding no             # Bloqueia agent forwarding

PermitOpen none                     # Bloqueia forwarding arbitr√°rio
PermitListen none                   # Bloqueia abrir portas remotas
GatewayPorts no                     # Bloqueia gateway ports
PermitTunnel no                     # Bloqueia interfaces tun/tap

# BLOCK 3: PERFORMANCE & TIMEOUTS
MaxAuthTries 4                      # Max 4 tentativas
LoginGraceTime 30                   # 30s para logar ou tchau
ClientAliveInterval 300             # Keepalive a cada 5 min
ClientAliveCountMax 2               # Cai se n√£o responder 2x
PrintMotd no                        # Menos spam no login
UseDNS no                           # Login r√°pido (sem reverse DNS lookup)

###############################################################################
### End of /etc/ssh/sshd_config.d/01_sshd_settings.conf ########################
###############################################################################
EOF

# Testa a configura√ß√£o antes de reiniciar (se der erro, N√ÉO reinicie)
sudo sshd -t

# Reinicia o servi√ßo SSH para aplicar
sudo systemctl restart ssh
```

> **Dica:** Mantenha sua sess√£o atual aberta. Abra **outro terminal** no seu PC e tente conectar (`ssh kvm2`). Se funcionar, sucesso! Se n√£o, voc√™ ainda tem a sess√£o aberta para corrigir.

## 10. Fail2Ban (Prote√ß√£o Brute-Force)

Mesmo sem senhas, logs de tentativas de acesso poluem o sistema e consomem recursos. O Fail2Ban bloqueia IPs que tentam conectar e falham repetidamente.

> **Nota do Autor:** Sim, eu sei. Estamos rodando Fail2Ban numa m√°quina que j√° tem DOIS firewalls (Borda + UFW) bloqueando a porta 22 para todo mundo, exceto meu IP. Isso se chama "paranoia saud√°vel" (ou exagero mesmo). Se um dia eu errar a config do firewall e abrir a porta sem querer, o Fail2Ban estar√° l√° rindo e banindo os bots. üòÇ

**Execute em TODAS as VPSs:**

```bash
# Defina o IP da sua casa/escrit√≥rio para nunca ser banido (whitelist)
export ADMIN_SSH_CIDR="187.108.118.25/32" # <-- Troque pelo SEU IP
export FAIL2BAN_IGNOREIP="$ADMIN_SSH_CIDR 127.0.0.1/8 ::1"

# Instala o Fail2Ban
sudo apt install fail2ban -y

# Cria a configura√ß√£o local (jail.local)
cat <<-EOF | sudo tee "/etc/fail2ban/jail.local"
[DEFAULT]

[sshd]
enabled = true
port = ssh
backend = systemd
# IPs ignorados (voc√™ e o localhost)
ignoreip = ${FAIL2BAN_IGNOREIP}

# Regras de banimento
maxretry = 5          # 5 tentativas falhas
findtime = 10m        # dentro de 10 minutos
bantime = 1h          # = Banido por 1 hora

# Banimento progressivo para reincidentes
bantime.increment = true
bantime.factor = 2    # Dobra o tempo a cada reincid√™ncia
bantime.max = 24h     # At√© o m√°ximo de 24h
EOF

# Reinicia o servi√ßo
sudo systemctl restart fail2ban
```

## 11. Firewall Local (UFW)

Defesa em profundidade. Se o firewall da Hostinger falhar ou for desativado, o UFW garante que ningu√©m acessa o que n√£o deve.

Aqui fazemos algo especial: **Liberamos o Swarm APENAS na interface VPN (`wg0`)**. Se algu√©m bater no IP p√∫blico tentando falar com o Docker Swarm, ser√° bloqueado.

**Execute em TODAS as VPSs:**

```bash
# Defina seu IP e a rede da VPN
export ADMIN_SSH_CIDR="187.108.118.25/32"    # <-- Troque pelo SEU IP
export WG_CIDR="10.100.0.0/24"              # Rede interna que usaremos no WireGuard
export WG_INTERFACE="wg0"                   # Interface do WireGuard

sudo apt install -y ufw

# Zera configura√ß√µes antigas (garante estado limpo)
sudo ufw disable
sudo ufw --force reset

# Pol√≠tica Padr√£o: Bloqueia tudo que entra, Libera tudo que sai
sudo ufw default deny incoming
sudo ufw default allow outgoing

# Libera SEU acesso SSH e WireGuard (UDP)
# DICA: Se seu IP n√£o √© fixo, libere para "Any" no SSH e confie nas chaves + Fail2Ban
# sudo ufw allow ssh comment "SSH Public"
sudo ufw allow from "$ADMIN_SSH_CIDR" to any port 22 proto tcp comment "SSH Admin"
sudo ufw allow from "$ADMIN_SSH_CIDR" to any port 51820 proto udp comment "WireGuard Admin"

# Libera tr√°fego do SWARM e NFS APENAS na interface da VPN (wg0)
# Ningu√©m de fora da VPN consegue tocar nesses servi√ßos
sudo ufw allow in on "$WG_INTERFACE" from "$WG_CIDR" to any port 2377 proto tcp comment "Swarm control (wg)"
sudo ufw allow in on "$WG_INTERFACE" from "$WG_CIDR" to any port 7946 proto tcp comment "Swarm gossip (wg)"
sudo ufw allow in on "$WG_INTERFACE" from "$WG_CIDR" to any port 7946 proto udp comment "Swarm gossip (wg)"
sudo ufw allow in on "$WG_INTERFACE" from "$WG_CIDR" to any port 4789 proto udp comment "Swarm vxlan (wg)"
sudo ufw allow in on "$WG_INTERFACE" from "$WG_CIDR" to any port 2049 proto tcp comment "NFSv4 (wg)"
```

**Execute APENAS no n√≥ `kvm8` (Edge/Traefik):**

```bash
# O kvm8 √© o √∫nico que recebe tr√°fego web p√∫blico
sudo ufw allow 80/tcp comment "HTTP public"
sudo ufw allow 443/tcp comment "HTTPS public"
```

**Finalizar e Ativar (EM TODAS):**

```bash
# Se voc√™ errou o IP do SSH l√° em cima, voc√™ vai cair agora.
sudo ufw --force enable
sudo ufw status verbose
```

## 12. Teste R√°pido de Firewall (HTTP)

Vamos subir um servidor web tempor√°rio em cada m√°quina para provar que nossas regras de firewall funcionam.

**Execute em todos os n√≥s (`kvm2`, `kvm4`, `kvm8`):**

```bash
mkdir test 
echo "<div style='display: grid; place-items: center; font-size: 5vw; height: 100vh;'>HELLO FROM $(hostname)</div>" > test/index.html
sudo python3 -m http.server -d test 80
```

**Resultado esperado:**
1. Abra `http://myswarm.cloud` (ou IP do `kvm8`) no navegador -> **FUNCIONA** (Voc√™ v√™ "HELLO FROM kvm8").
2. Abra `http://inprod.cloud` (ou IP do `kvm2`) -> **FALHA** (Timeout/Recusado).
3. Abra `http://otaviomiranda.cloud` (ou IP do `kvm4`) -> **FALHA** (Timeout/Recusado).

**Para parar:**
Pressione `Ctrl+C` no terminal e remova a pasta de teste:
```bash
rm -r test
```

## 13. WireGuard (VPN Privada)

Essa √© a parte mais trabalhosa, mas essencial. Vamos criar uma rede privada (`10.100.0.0/24`) onde os n√≥s conversar√£o de forma segura e criptografada, sem expor o Swarm na internet p√∫blica.

**Execute em cada VPS, AJUSTANDO a vari√°vel `WG_IP` para cada uma:**

```bash
# === PASSO 1: Instala√ß√£o e Gera√ß√£o de Chaves ===

# Instala o WireGuard
sudo apt install -y wireguard

# Defina a interface e o IP DESTA M√ÅQUINA (Ajuste para cada VPS!)
export WG_INTERFACE="wg0"
export WG_CIDR="24"
# kvm2=10.100.0.2 | kvm4=10.100.0.4 | kvm8=10.100.0.8
export WG_IP="10.100.0.2" # <--- üö® TROQUE ISSO EM CADA M√ÅQUINA

# Configura caminhos
export WG_DIR="/etc/wireguard"
export WG_CONF="$WG_DIR/$WG_INTERFACE.conf"
export WG_PRI="${WG_DIR}/private.key"
export WG_PUB="${WG_DIR}/public.key"

# Gera chaves e arquivo de config se n√£o existirem
if ! sudo test -f "${WG_PRI}"; then
    echo "Gerando chaves..."
    sudo install -d -m 0700 -o root -g root "${WG_DIR}"
    sudo wg genkey | sudo tee "${WG_PRI}" | sudo wg pubkey | sudo tee "${WG_PUB}" > /dev/null
    sudo chmod 600 "${WG_PRI}"
    sudo chmod 644 "${WG_PUB}"
fi

# L√™ as chaves para vari√°veis
WG_PRI_VALUE=$(sudo cat "${WG_PRI}")
WG_PUB_VALUE=$(sudo cat "${WG_PUB}")

echo "Sua Public Key: ${WG_PUB_VALUE}"

# Cria o arquivo de configura√ß√£o inicial (com Peers comentados)
cat <<-EOF | sudo tee $WG_CONF
[Interface]
Address = $WG_IP/$WG_CIDR
ListenPort = 51820
PrivateKey = ${WG_PRI_VALUE}

# --- LEMBRETE DE PEERS (MAPA) ---
# kvm2 -> 10.100.0.2 (Public IP: 76.13.71.178)
# kvm4 -> 10.100.0.4 (Public IP: 191.101.70.130)
# kvm8 -> 10.100.0.8 (Public IP: 89.116.73.152)

# [Peer]
# PublicKey = <CHAVE_PUBLICA_DO_OUTRO_VPS>
# AllowedIPs = <IP_INTERNO_DO_OUTRO_VPS>/32
# Endpoint = <IP_PUBLICO_DO_OUTRO_VPS>:51820
# PersistentKeepalive = 25
EOF

# Habilita e inicia o servi√ßo
sudo systemctl enable wg-quick@wg0
sudo systemctl restart wg-quick@wg0
sudo wg show
```

### Passo 2: O " troca-troca" de chaves (Manual)

Agora vem a parte manual. Voc√™ precisa editar o arquivo `/etc/wireguard/wg0.conf` em cada m√°quina e adicionar os blocos `[Peer]` das **outras duas m√°quinas**.

Use `sudo wg show` em cada terminal para ver a Public Key de cada um e monte o quebra-cabe√ßa.

**Exemplo de como deve ficar o arquivo no `kvm2`:**
```ini
[Interface]
Address = 10.100.0.2/24
...

[Peer] # kvm4
PublicKey = <PUBLIC_KEY_DO_KVM4>
AllowedIPs = 10.100.0.4/32
Endpoint = 191.101.70.130:51820
PersistentKeepalive = 25

[Peer] # kvm8
PublicKey = <PUBLIC_KEY_DO_KVM8>
AllowedIPs = 10.100.0.8/32
Endpoint = 89.116.73.152:51820
PersistentKeepalive = 25
```

Ap√≥s editar, aplique as mudan√ßas:
```bash
sudo systemctl restart wg-quick@wg0
sudo wg show
```

**Teste de Ping (Fundamental):**
Do `kvm2`, tente pingar os IPs internos dos outros:
```bash
ping 10.100.0.4
ping 10.100.0.8
```
Se pingar, parab√©ns! Sua rede privada criptografada est√° de p√©.

## 14. Configura√ß√£o do NFS (Storage Compartilhado)

Como estamos em um cluster, precisamos de um local comum para que arquivos (como os jobs do webhook) sejam vistos por todos, independente de onde o servi√ßo esteja rodando. O `kvm8` ser√° nosso servidor de arquivos.

### Servidor NFS (Apenas no kvm8)

Configuramos o servidor com permiss√µes restritas (UID/GID 1011) para alinhar com o usu√°rio que rodar√° dentro dos containers.

```bash
# Instala o servidor
sudo apt-get install -y nfs-kernel-server

# Cria os diret√≥rios
sudo mkdir -p /srv/nfs/swarm_data/webhook_jobs

# === Permiss√µes e Grupos ===
# Cria grupo 'app' com GID 1011 (mesmo ID usado no container da API)
sudo groupadd -g 1011 app || true
# Adiciona seu usu√°rio ao grupo para voc√™ poder gerenciar arquivos
sudo usermod -aG app "$USER" || true

# Aplica permiss√µes
sudo chown -R root:app /srv/nfs/swarm_data/webhook_jobs
sudo chmod 2770 /srv/nfs/swarm_data/webhook_jobs
# ACLs para garantir que novos arquivos herdem as permiss√µes
sudo setfacl -R -m g:app:rwx /srv/nfs/swarm_data/webhook_jobs
sudo setfacl -R -m d:g:app:rwx /srv/nfs/swarm_data/webhook_jobs

# Aplica mudan√ßas de grupo na sess√£o atual
newgrp app

# === Exporta√ß√£o (Compartilhamento) ===
# Define a regra de exporta√ß√£o para a rede VPN (10.100.0.0/24)
# all_squash + anonuid=1011: For√ßa tudo a ser escrito como usu√°rio 'app'
EXPORT_LINE="/srv/nfs/swarm_data 10.100.0.0/24(rw,sync,no_subtree_check,all_squash,anonuid=1011,anongid=1011,fsid=0)"
sudo grep -qF "$EXPORT_LINE" /etc/exports || echo "$EXPORT_LINE" | sudo tee -a /etc/exports

# Aplica e verifica
sudo exportfs -ra
sudo exportfs -v
```

### Clientes NFS (kvm2, kvm4 e tamb√©m no kvm8)

Todos os n√≥s precisam montar essa pasta. Sim, inclusive o `kvm8` monta a pr√≥pria pasta via rede (loopback/VPN) para garantir que o caminho `/mnt/nfs` seja id√™ntico em todo o cluster.

```bash
# Instala o cliente
sudo apt-get install -y nfs-common
sudo mkdir -p /mnt/nfs

# Cria o grupo localmente para alinhar permiss√µes (opcional, mas recomendado)
sudo groupadd -g 1011 app || true
sudo usermod -aG app "$USER" || true

# Configura a montagem autom√°tica no Boot (/etc/fstab)
# Destaque para x-systemd.requires=wg-quick@wg0.service:
# S√≥ tenta montar DEPOIS que a VPN subir.
echo "10.100.0.8:/ /mnt/nfs nfs4 rw,vers=4.2,_netdev,noatime,nofail,x-systemd.automount,x-systemd.idle-timeout=600,x-systemd.device-timeout=10s,x-systemd.mount-timeout=30s,x-systemd.requires=wg-quick@wg0.service 0 0" | sudo tee -a /etc/fstab

sudo systemctl daemon-reload

# Monta tudo
sudo mount -a

# Verifica se montou
findmnt /mnt/nfs
```

### Valida√ß√£o Final de Permiss√µes
Teste se conseguimos escrever na pasta compartilhada (como se fossemos o app). Execute em qualquer n√≥:

```bash
# Tenta criar um arquivo de teste
mktemp -p /mnt/nfs/webhook_jobs perm_test.XXXXXX

# Verifica permiss√µes
ls -ld /mnt/nfs/webhook_jobs

# Limpa sujeira
sudo rm -f /mnt/nfs/webhook_jobs/perm_test.*
```

## 15. Deploy Keys (Acesso ao Git)

Precisamos baixar o c√≥digo do projeto nas VPSs. Para n√£o usar sua senha pessoal, usaremos "Deploy Keys" (chaves SSH espec√≠ficas para leitura do reposit√≥rio).

**Execute em cada VPS (`kvm2`, `kvm4`, `kvm8`):**

1.  **Gere a chave de acesso:**
    ```bash
    ssh-keygen -t ed25519
    # Pressione ENTER para todas as perguntas (local padr√£o, sem senha)
    ```

2.  **Pegue a chave p√∫blica:**
    ```bash
    cat ~/.ssh/id_ed25519.pub
    ```
    *Copie o conte√∫do que aparece (come√ßa com `ssh-ed25519 ...`).*

3.  **Adicione no GitHub:**
    *   V√° no seu reposit√≥rio -> **Settings** -> **Deploy Keys**.
    *   Clique em **Add deploy key**.
    *   **Title:** `kvm2` (ou o nome do VPS).
    *   **Key:** Cole a chave p√∫blica.
    *   **Allow write access:** Deixe desmarcado (somente leitura √© mais seguro).
    *   Clique em **Add key**.

*Repita para todas as 3 m√°quinas.*

## 16. Clone do Reposit√≥rio

Agora trazemos o c√≥digo para dentro dos servidores. Usaremos `/opt/dockerswarmp1` como padr√£o.

**Execute em TODAS as VPSs:**

```bash
# Cria o diret√≥rio e ajusta permiss√£o para seu usu√°rio
sudo mkdir -p /opt/dockerswarmp1
sudo chown -R "$USER:$USER" /opt/dockerswarmp1

# Clona o reposit√≥rio (use a URL SSH para usar a Deploy Key)
# üö® IMPORTANTE: Use o SEU reposit√≥rio aqui
git clone git@github.com:luizomf/dockerswarmp1.git /opt/dockerswarmp1
```

> **Verifica√ß√£o:** Rode `ls /opt/dockerswarmp1` e veja se os arquivos apareceram.

## 17. Inicializando o Swarm

Agora unimos as m√°quinas em um cluster. Usaremos os IPs da VPN (`10.100.0.x`) para que o tr√°fego de gest√£o do Swarm passe dentro do t√∫nel criptografado.

**1. Iniciar no L√≠der (Execute no `kvm8`):**
```bash
# --advertise-addr garante que o Swarm use o IP da VPN
docker swarm init --advertise-addr 10.100.0.8
```
*Copie o comando `docker swarm join ...` que vai aparecer na tela.*

**2. Adicionar os N√≥s (Execute no `kvm2` e `kvm4`):**
Cole o comando que voc√™ copiou. Deve ser parecido com:
```bash
docker swarm join --token SWMTKN-1-xxxxx 10.100.0.8:2377
```

**3. Promover a Gerentes (Execute no `kvm8`):**
Por padr√£o, os novos n√≥s entram como "Workers". Vamos promov√™-los a "Managers" para ter alta disponibilidade (se o kvm8 cair, outro assume a gest√£o).
```bash
docker node promote kvm2 kvm4
```

**4. Validar o Cluster (Execute no `kvm8`):**
```bash
docker node ls
```
O resultado esperado √© ver 3 n√≥s, todos com `MANAGER STATUS` preenchido (Leader + Reachable).

```text
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS
ikfyqsoeqxybu...              kvm2       Ready     Active         Reachable
9dxwy9io1zuan...              kvm4       Ready     Active         Reachable
jo48gk4elvo4l... *            kvm8       Ready     Active         Leader
```

## 18. Vari√°veis de Ambiente (.env)

O `just` (nosso task runner) precisa saber qual dom√≠nio usar e outros detalhes. Vamos configurar o `.env` apenas no n√≥ que usaremos para fazer o deploy (geralmente o `kvm8`, mas como todos s√£o managers, pode ser em qualquer um).

**Execute no `kvm8`:**

```bash
cd /opt/dockerswarmp1
cp .env.example .env
vim .env
```

**O que voc√™ DEVE mudar:**
1. `CURRENT_ENV`: mude para `production` (isso ativa o TLS real no Traefik).
2. `EMAIL`: seu e-mail para o Let's Encrypt.
3. `APP_DOMAIN`: o dom√≠nio que voc√™ apontou para o `kvm8` (ex: `myswarm.cloud` ou `app.myswarm.cloud`).
4. `GITHUB_WEBHOOK_SECRET`: Gere um hash (`python3 -c "import secrets; print(secrets.token_hex(32))"`) e guarde. Usaremos o mesmo no GitHub.
5. `POSTGRES_PASSWORD`: Use `ANY_VALUE` aqui, pois a senha real vir√° de um **Docker Secret** no pr√≥ximo passo.

**Exemplo:**
```bash
CURRENT_ENV=production
EMAIL="seu@email.com"
APP_DOMAIN="myswarm.cloud"
GITHUB_WEBHOOK_SECRET="f6a7d8..."
```

## 19. Redes e Labels (Arquitetura)

Antes do deploy, precisamos preparar o terreno:
1.  **Labels:** Marcar o `kvm8` para que o Swarm saiba que este √© o n√≥ "Especial" (onde ficar√£o o Banco de Dados e o Traefik).
2.  **Redes:** Criar as redes Overlay (que funcionam sobre o WireGuard).

**Execute no `kvm8`:**

```bash
cd /opt/dockerswarmp1

# === LABEL ===
# Marca o n√≥ atual como 'kvm8'.
# Nossos servi√ßos 'postgres' e 'traefik' t√™m uma regra: "Rode apenas onde role == kvm8"
docker node update --label-add role=kvm8 kvm8

# === REDES ===
# 'public': Rede onde o Traefik escuta e roteia o tr√°fego externo.
docker network create --driver=overlay --attachable public

# 'internal': Rede fechada onde API e Banco conversam. Sem acesso externo direto.
docker network create --driver=overlay --attachable --internal internal
```


















